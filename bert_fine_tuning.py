# -*- coding: utf-8 -*-
"""BERT_Fine_Tuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12F3duYkj--ZPPuPvDFSr8J6JBLF5JPx2
"""

!pip install transformers datasets evaluate

from google.colab import drive
drive.mount('/content/drive')

from transformers import pipeline
import json
import os
os.chdir('/content/drive/Shareddrives/CS542_Competition/Fine_Tuning')

!ls

from datasets import load_dataset

#No longer loading from the json files, I copied the csv from the drive.

train_dataset = load_dataset("csv", data_files='autocast_train_questions_combined_with_tf_negated.csv')
test_dataset = load_dataset('csv', data_files='test_dataset.csv')

#When extracting the data at first, it was in a DatasetDict format. easier to deal with just the Dataset object.
#need to split train dataset into train and validation, originally thought the model would ask for test set.

train_dataset = train_dataset['train']
test_dataset = test_dataset['train']

#Shouldn't need these columns, removing them.

train_dataset = train_dataset.remove_columns(['publish_time', 'close_time', 'source_links', 'prediction_count', 'forecaster_count', 'crowd', 'question_negated'])
test_dataset = test_dataset.remove_columns([ 'publish_time', 'close_time', 'tags'])

for i, row in enumerate(train_dataset):
    if row['id'] == 'G2040':
        # print the index, question, and answer for the row where id is 'G2'
        print(i, row['question'], row['answer'])
        break  # exit the loop since we found the row we're looking for

#double checking what the data looks like afterwards.

train_dataset[2]

test_dataset[0]

from transformers import AutoTokenizer, DefaultDataCollator

model = 'distilbert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model)
data_collator = DefaultDataCollator()

def preprocess(entry):

    """
        Modify background field to include choices and answers. Any that come up with an error are skipped, like the numerical questions.
        This'll need to be addressed. Trying to use 1 preprocess function to simplify finding the answer in the context.
    """

    entry['background'] = f"{entry['background']} Choices: {entry['choices']}"
    answer = ''
    if entry['qtype'] == 'mc':
        choice_split = entry['choices'].rstrip("']")
        choice_split = choice_split.replace("', '", '@').split('@')
        try:
            if entry['answer'] == 'A':
                answer = choice_split[0][2:]
            elif entry['answer'] == 'B':
                answer = choice_split[1]
            elif entry['answer'] == 'C':
                answer = choice_split[2]
            elif entry['answer'] == 'D':
                answer = choice_split[3]

        except:
            pass

    elif entry['qtype'] == 't/f':
        try:
            answer = entry['answer']

        except:
            pass

     #may need some logic for numerical questions.
     #-------Tokenizer and context modification in the same function----------

    inputs = tokenizer(
        entry['question'],
        entry["background"],
        max_length=512,
        truncation="only_second",
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")

    start_positions = []
    end_positions = []

    context = entry['background']

    try:
        start = context.rfind(answer)
        end = start + len(answer)

    except:
        start, end = 0,0

    start_positions.append(start)
    end_positions.append(end)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions

    #double check that it is searching for the correct answer in the background

    # print(start)
    # print(context)

    # ans = ''
    # for i in range(start,end):
    #     ans += context[i]

    # print(ans)

    return inputs


#tokenized_train_dataset = train_dataset.map(preprocess, remove_columns=['tags', 'status', 'qtype', 'Unnamed: 0'])
#tokenized_test_dataset = test_dataset.map(preprocess, remove_columns=['Unnamed: 0', 'qtype'])

#tokenized_train_dataset[0]

print(preprocess(train_dataset[2]))

#updated_test_dataset[0]

# #Rami
# # Add answers in question
# question_list = []

# for data in train_dataset:
#     if data["qtype"] == 'mc' or data["qtype"]=='t/f':
#       question = data["question"]
#       choices = eval(data["choices"])  # Convert string to list
#       answer = data["answer"]

#       question += " Answer: "
#       for i, choice in enumerate(choices):
#         question += f"{chr(i+65)}) {choice}, "

#       question_list.append(question)

#       # for ans in choices:
#       #   if answer == 'A':
#       #     data["answer"] = "."+ans+"."
#       #   if answer == 'B':
#       #     data["answer"] = "."+ans+"."
#       #   if answer == 'C':
#       #     data["answer"] = "."+ans+"."
#       #   if answer == 'D':
#       #     data["answer"] = "."+ans+"."

#     else:
#       question_list.append("")


# new_train_dataset = []
# for data, question in zip(train_dataset, question_list):
#     data["question"] = question
#     new_train_dataset.append(data)

# train_dataset = new_train_dataset
# print(train_dataset[0])

# #Rami
# # Add answers in background
# def add_context(entry):
#   back_list = []
#   ans_list=[]

#   for data in train_dataset:
#     # if data["qtype"] == 'mc' or data["qtype"]=='t/f':
#       back = data["background"]
#       answer = data["answer"]

#       if answer == None:
#         answer= "Nothing"
#       if back == None:
#         back=""

#       choices = data["choices"]

#       if isinstance(choices, str):
#         choices = eval(data["choices"])  # Convert string to list
#         for i, choice in enumerate(choices):
#           back += f"{chr(i+65)}) {choice}, "

#       back +="."+answer+"."
#       back_list.append(back)
#       ans_list.append(answer)


#     # else:
#     #   back_list.append("")



#   new_train_dataset = []
#   for data, b, a in zip(train_dataset, back_list,ans_list):
#     data['background'] = b
#     data['answer']= a
#     new_train_dataset.append(data)

# train_dataset = new_train_dataset

# updated_train_dataset = train_dataset.map(add_context, remove_columns=['tags', 'status', 'choices', 'qtype', 'Unnamed: 0'])
# updated_test_dataset = test_dataset.map(add_context, remove_columns=['Unnamed: 0', 'qtype', 'choices'])
# print(train_dataset[1661])

# def preprocess_function(examples):

#     """
#         Apply tokenizer to the background data, converts text into tokens. Question answering trainer requires the start and end positions of the answer, which must be in the
#         background data. For now, the choices and answers were added to the background in the add_context() function. Trying
#         to offer the start and end positions of the correct choice.
#         Still need to work on this, leaving as is for now to try and get the model to train.
#     """

#     #questions = [q.strip() for q in examples["question"]]

#     inputs = tokenizer(
#         examples['question'],
#         examples["background"],
#         max_length=256,
#         truncation="only_second",
#         return_offsets_mapping=True,
#         padding="max_length",
#     )

#     offset_mapping = inputs.pop("offset_mapping")

#     start_positions = []
#     end_positions = []

#     context = examples['background']
#     answer_choice = examples['answer']

#     if answer_choice is None:
#         print(examples)
#         print(answer_choice)
#         print(i)

#     answer_index = context.rfind(answer_choice)

#     start = context.rfind('.', 0, answer_index) + 1
#     end = context.find('.', answer_index) - 1

#     start_positions.append(start)
#     end_positions.append(end)

#     inputs["start_positions"] = start_positions
#     inputs["end_positions"] = end_positions

#     return inputs


# # not sure what this does
# tokenized_train_dataset = updated_train_dataset.map(preprocess_function) #remove_columns=['answer'])
# tokenized_test_dataset = updated_test_dataset.map(preprocess_function)



# #preprocess_function(updated_train_dataset)
# #tokenized_test_dataset[5]

from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer

#the question answering model is actually: https://huggingface.co/distilbert-base-cased-distilled-squad , but wanted to try this automodel first.

model = AutoModelForQuestionAnswering.from_pretrained("distilbert-base-uncased")

from sklearn.metrics import accuracy_score

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc}

training_args = TrainingArguments(

    output_dir="distilbert_fine_tuning_test",

    evaluation_strategy="epoch",

    learning_rate=2e-5,

    per_device_train_batch_size=16,

    per_device_eval_batch_size=16,

    num_train_epochs=3,

    weight_decay=0.01,

    gradient_accumulation_steps=8,

    eval_steps = 32,

    logging_steps = 32,

    #label_names = 'answer'


)

trainer = Trainer(

    model=model,

    args=training_args,

    train_dataset=tokenized_train_dataset,

    eval_dataset=tokenized_test_dataset,

    tokenizer=tokenizer,

    data_collator=data_collator,

    #compute_metrics=compute_metrics,


)

trainer.train()

#save model
trainer.save_model("distilbert_tuned_model_YL_4-6")

#Pipeline test

question_answerer = pipeline(task = "question-answering", model = "distilbert_tuned_model_YL_4-6")

text = r"""
    Amid large-scale economic protests, calls for reform of Chileâ€™s pension system have grown (Financial Times, AP, IPE, El Universal [in Spanish]).
    A constitutional amendment to the same effect would count. Legislation enabling or delegating new regulatory authority to cap administrative fees
    and/or operating profits would count.Please note that this question is a companion to Question #1417, which closes in 2020 (Question #1417). We will
    be analyzing the differences between forecasts with the different closing dates. Choices: ['yes', 'no']
"""


questions = [
        "Before 1 July 2021, will the Chilean government not pass legislation that caps administrative fees and/or operating profits of the country's pension fund managers? Choices: ['yes', 'no']",
        "Before 1 July 2021, will the Chilean government pass legislation that caps administrative fees and/or operating profits of the country's pension fund managers? Choices: ['yes', 'no']"
 ]


for question in questions:
  result = question_answerer(question=question, context=text)
  print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}")

text = r"""
    China suspended initial public offerings (IPOs) in early July
    (http://www. bloomberg. com/news/articles/2015-07-04/china-stock-brokers-set-up-19-billion-fund-to-stem-market-rout , http://www. reuters. com/article/2015/07/05/us-china-markets-brokerage-pledge-idUSKCN0PE08E20150705 , http://www. wsj. com/articles/china-setting-up-fund-to-stabilize-stock-market-1435991611 ).
    Choices: ['yes', 'no']
"""


questions = [
        "Will there be an initial public offering on either the Shanghai Stock Exchange or the Shenzhen Stock Exchange before 1 January 2016?"
 ]


for question in questions:
  result = question_answerer(question=question, context=text)
  print(f"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}")